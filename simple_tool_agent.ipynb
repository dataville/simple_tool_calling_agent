{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e0f555-77f1-4748-8279-587f738c760e",
   "metadata": {},
   "source": [
    "# ü§ñ Building a Tool-Calling AI Agent with LangChain & LangGraph\n",
    "## A Step-by-Step Guide to Understanding How AI Agents Make Decisions and Use Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09390009-f937-4259-97b6-abb6eb3552ce",
   "metadata": {},
   "source": [
    "#vüìö What This Notebook Demonstrates\n",
    "This notebook provides a complete, hands-on walkthrough of building a real AI agent that can autonomously decide when and how to use tools. Unlike simple chatbots that only generate text, this agent can:\n",
    "\n",
    "## Key Concepts You'll Learn:\n",
    "\n",
    "* üß† Autonomous Decision Making: How AI agents analyze user requests and decide whether they need to use external tools or can answer directly\n",
    "üîß Tool Integration: How to give your AI agent \"abilities\" through custom tools that extend its capabilities beyond just conversation\n",
    "* üîÑ Workflow Orchestration: How LangGraph creates a decision flow that allows the agent to think, act, observe results, and respond intelligently\n",
    "* üèóÔ∏è Modular Architecture: How to structure agent systems with clear separation between thinking (LLM), acting (tools), and routing (graph)\n",
    "\n",
    "## What We Build:\n",
    "A functional weather assistant agent that:\n",
    "\n",
    "1. Understands natural language requests like \"What's the weather in Boston?\"\n",
    "2. Decides it needs to use the weather tool (not just make up information)\n",
    "3. Executes the tool with the correct parameters\n",
    "4. Synthesizes the tool's output into a natural, helpful response\n",
    "\n",
    "## Why This Matters:\n",
    "This pattern is the foundation for more complex AI systems that can:\n",
    "\n",
    "* Search databases and APIs\n",
    "* Perform calculations and data analysis\n",
    "* Interact with external systems\n",
    "* Chain multiple actions together to solve complex problems\n",
    "* Make intelligent decisions about what tools to use and when\n",
    "\n",
    "By the end of this notebook, you'll understand not just how to build an AI agent, but why each component exists and how they work together to create truly autonomous AI systems that can take actions on behalf of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850094a9-040f-46e8-9d3d-e978fe6d7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29f7d5-072a-4820-8270-f1ac94d9f941",
   "metadata": {},
   "source": [
    "# ü§ñ Connecting to the Language Model\n",
    "This code establishes a connection to our AI agent's \"brain\" - the Large Language Model (LLM) that will process requests and make decisions about when to use tools.\n",
    "\n",
    "## What's happening here:\n",
    "```python\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8082/v1\",  # Points to local vLLM server instead of OpenAI\n",
    "    api_key=\"not-needed\",                  # vLLM doesn't require authentication\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\"  # The actual model running on vLLM\n",
    ")\n",
    "```\n",
    "\n",
    "## Key Points:\n",
    "\n",
    "* ChatOpenAI class: Even though we're using Mistral (not OpenAI), LangChain's ChatOpenAI class works because vLLM provides an OpenAI-compatible API endpoint. This is a common pattern - many LLM servers mimic OpenAI's API format for compatibility.\n",
    "* base_url: Instead of connecting to OpenAI's servers, we're pointing to localhost:8082 where vLLM is running locally. This means the model is running on your own machine, giving you full control and privacy.\n",
    "* api_key=\"not-needed\": Unlike OpenAI's API which requires authentication, our local vLLM server doesn't need an API key. We still have to provide this parameter because the ChatOpenAI class expects it.\n",
    "* model: Specifies Mistral-7B-Instruct v0.3, a powerful open-source model that's been fine-tuned to follow instructions and engage in helpful dialogue. This model will power our agent's reasoning and decision-making.\n",
    "\n",
    "## üí° Think of this as: Creating a direct phone line to an AI assistant that lives on your computer rather than in the cloud. The agent will use this connection to think through problems and decide when to use tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f560d2b2-b2ad-44b8-9ac5-5804d72a99ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to vLLM\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8082/v1\",\n",
    "    api_key=\"not-needed\",\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ffa315-1c7b-487f-92b7-1a0c05b0e4b9",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Creating a Tool for the Agent\n",
    "This code defines a tool that our AI agent can choose to use when it needs weather information. Think of tools as special abilities or functions the agent can call upon when needed.\n",
    "\n",
    "## What's happening here:\n",
    "```python\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves current weather information for a specified location.\n",
    "    \n",
    "    Args:\n",
    "        location: The city name and optionally state/country (e.g., \"San Francisco, CA\" or \"London, UK\")\n",
    "    \n",
    "    Returns:\n",
    "        A string containing the current temperature in Fahrenheit and weather conditions.\n",
    "    \"\"\"\n",
    "    return f\"Current weather in {location}: Temperature is 72¬∞F, conditions are sunny with clear skies\"\n",
    "```\n",
    "\n",
    "## Key Components:\n",
    "\n",
    "* @tool decorator: This special decorator from LangChain transforms a regular Python function into a tool that the AI agent can discover and use. It's like registering this function in the agent's toolbox.\n",
    "* Type hints (location: str -> str): These tell the agent exactly what type of input the tool expects and what it returns. This helps the LLM make correct tool calls.\n",
    "* The docstring is CRITICAL:\n",
    "\n",
    "    * The agent reads this description to understand when and how to use the tool\n",
    "    * Without a clear docstring, the agent might not know this tool exists or how to use it properly\n",
    "    * The docstring acts as an \"instruction manual\" for the AI\n",
    "\n",
    "\n",
    "* Mock implementation: Currently returns hardcoded weather data (always 72¬∞F and sunny). In a production system, this would make an actual API call to a weather service.\n",
    "\n",
    "## How the Agent Uses This Tool:\n",
    "\n",
    "1. User asks: \"What's the weather in Boston?\"\n",
    "2. Agent reads the tool's docstring and thinks: \"This tool can get weather for a location\"\n",
    "3. Agent decides: \"I need to use the get_weather tool with 'Boston' as the location\"\n",
    "4. Agent calls: get_weather(\"Boston\")\n",
    "5. Agent receives: The weather data and formats it into a natural response\n",
    "\n",
    "## üí° Important: The quality of your docstring directly impacts how well the agent can use your tool. Always write clear, detailed descriptions that explain what the tool does and what parameters it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1253cea-adab-4970-888e-6fd45151748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool with proper docstring\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves current weather information for a specified location.\n",
    "    \n",
    "    Args:\n",
    "        location: The city name and optionally state/country (e.g., \"San Francisco, CA\" or \"London, UK\")\n",
    "    \n",
    "    Returns:\n",
    "        A string containing the current temperature in Fahrenheit and weather conditions.\n",
    "    \"\"\"\n",
    "    return f\"Current weather in {location}: Temperature is 72¬∞F, conditions are sunny with clear skies\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e6f64-d405-47c9-b2e7-0bc7ec380a93",
   "metadata": {},
   "source": [
    "# üîó Connecting Tools to the Language Model\n",
    "This code gives our AI agent access to the tools we've created, enabling it to discover and use them during conversations.\n",
    "\n",
    "## What's happening here:\n",
    "```python\n",
    "tools = [get_weather]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "```\n",
    "\n",
    "## Breaking it down:\n",
    "\n",
    "* tools = [get_weather]: Creates a list of all available tools. In this case, we only have one tool (get_weather), but you could add more:\n",
    "\n",
    "```python \n",
    "tools = [get_weather, search_web, send_email, calculate_math]\n",
    "```\n",
    "\n",
    "* llm.bind_tools(tools): This is where the magic happens! The bind_tools() method:\n",
    "\n",
    "    * Tells the LLM what tools are available\n",
    "    * Automatically extracts tool descriptions from the docstrings\n",
    "    * Configures the model to output specially formatted tool calls when needed\n",
    "    * Returns a new LLM instance that's \"tool-aware\"\n",
    "\n",
    "\n",
    "\n",
    "## What changes after binding:\n",
    "Before binding (llm):\n",
    "\n",
    "* Can only respond with text\n",
    "* Doesn't know about any tools\n",
    "* Would just say \"I can't check the weather\"\n",
    "\n",
    "## After binding (llm_with_tools):\n",
    "\n",
    "* Knows about the get_weather tool\n",
    "* Can decide when to use it\n",
    "* Can format proper tool calls with the right parameters\n",
    "* Can chain tool usage with responses\n",
    "\n",
    "## üí° Think of it like this: Binding tools is like giving someone a Swiss Army knife and teaching them what each tool does. Now when they encounter a problem, they can choose the right tool for the job instead of just talking about it.\n",
    "## Under the hood:\n",
    "When you bind tools, LangChain actually modifies the system prompt to include information about available tools and how to call them. The LLM will now respond with special structured outputs when it wants to use a tool, which LangGraph can intercept and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f273c573-87a5-43c1-8557-f08828d5f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind tools\n",
    "tools = [get_weather]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6191e6e-725b-428d-afb7-23a2d6cd73c1",
   "metadata": {},
   "source": [
    "# üéØ The Agent's Decision-Making Function\n",
    "This function is the core of our agent - it's where the AI receives messages, thinks about them, and decides whether to respond directly or use a tool.\n",
    "\n",
    "## What's happening here:\n",
    "```python\n",
    "def call_model(state: MessagesState):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "## Breaking down each part:\n",
    "\n",
    "* state: MessagesState: The current conversation state\n",
    "\n",
    "    * Contains the full message history (user messages, AI responses, tool results)\n",
    "    * LangGraph automatically maintains and passes this state between nodes\n",
    "    * Think of it as the agent's \"memory\" of the conversation\n",
    "\n",
    "\n",
    "* llm_with_tools.invoke(state[\"messages\"]): This is where the thinking happens\n",
    "\n",
    "    * Sends the entire conversation history to the LLM\n",
    "    * The LLM analyzes the latest message and decides what to do\n",
    "    * Returns either:\n",
    "\n",
    "        * A regular text response, or\n",
    "        * A special tool-call request (e.g., \"I need to use get_weather for Boston\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* return {\"messages\": [response]}: Updates the conversation state\n",
    "\n",
    "    * Wraps the response in a list and dictionary format that LangGraph expects\n",
    "    * Adds the new response to the conversation history\n",
    "    * This updated state flows to the next node in the graph\n",
    "\n",
    "\n",
    "\n",
    "## How it works in practice:\n",
    "\n",
    "1. User says: \"What's the weather in Boston?\"\n",
    "2. Function receives: All previous messages plus this new one\n",
    "3. LLM analyzes: \"Hmm, they want weather info, I have a weather tool...\"\n",
    "4. Function returns: Either a tool call or direct response\n",
    "5. LangGraph routes: Based on the response type, goes to tools node or ends\n",
    "\n",
    "## üí° Key insight: \n",
    "This function doesn't actually execute tools - it just decides whether tools are needed. The actual tool execution happens in the **ToolNode** that we'll see next. This separation of concerns makes the agent modular and easy to extend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eef184c-b70c-4d9f-ba4e-afd25fca712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920a2c5-31a0-4472-bb01-e9b5e5bfe4b0",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è Building the Agent's Workflow Graph\n",
    "This code constructs the \"brain map\" of our agent - defining how it flows between thinking, using tools, and responding. LangGraph uses a graph structure where nodes are functions and edges are the paths between them.\n",
    "\n",
    "## What's happening here:\n",
    "```python \n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(\"agent\", call_model)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools))\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    lambda x: \"tools\" if x[\"messages\"][-1].tool_calls else \"__end__\"\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"agent\")\n",
    "graph_builder.set_entry_point(\"agent\")\n",
    "graph = graph_builder.compile()\n",
    "```\n",
    "\n",
    "**Step-by-step breakdown:**\n",
    "\n",
    "1. **`StateGraph(MessagesState)`**: Creates a new graph that tracks conversation messages\n",
    "   - Think of this as drawing an empty flowchart\n",
    "   - `MessagesState` defines what kind of data flows through the graph\n",
    "\n",
    "2. **`add_node(\"agent\", call_model)`**: Adds the agent's thinking node\n",
    "   - Names it \"agent\" \n",
    "   - Links it to our `call_model` function\n",
    "   - This is where the LLM makes decisions\n",
    "\n",
    "3. **`add_node(\"tools\", ToolNode(tools))`**: Adds the tool execution node\n",
    "   - `ToolNode` is a pre-built component that knows how to execute tools\n",
    "   - It receives tool calls from the agent and runs the actual functions\n",
    "\n",
    "4. **`add_conditional_edges(...)`**: Creates smart routing from the agent node\n",
    "   - Checks if the last message contains tool calls\n",
    "   - If yes ‚Üí routes to \"tools\" node\n",
    "   - If no ‚Üí routes to \"__end__\" (conversation complete)\n",
    "   - This is the decision point: \"Should I use a tool or just respond?\"\n",
    "\n",
    "5. **`add_edge(\"tools\", \"agent\")`**: Always return to agent after using a tool\n",
    "   - After executing a tool, go back to the agent\n",
    "   - The agent can then use the tool's output to formulate a response\n",
    "\n",
    "6. **`set_entry_point(\"agent\")`**: Defines where conversations start\n",
    "   - Every new message first goes to the agent node\n",
    "\n",
    "7. **`graph.compile()`**: Finalizes and optimizes the graph for execution\n",
    "\n",
    "### Visual representation of the flow:\n",
    "```\n",
    "User Message\n",
    "     ‚Üì\n",
    "  [AGENT] ‚Üê (starts here)\n",
    "     ‚Üì\n",
    "  Decides: Need tool?\n",
    "   ‚Üô        ‚Üò\n",
    " Yes         No\n",
    "  ‚Üì           ‚Üì\n",
    "[TOOLS]    [END]\n",
    "  ‚Üì\n",
    "Back to AGENT\n",
    "  ‚Üì\n",
    "[Response to User]\n",
    "\n",
    "\n",
    "üí° The beauty of this design: The agent can chain multiple tool calls if needed. For example, if asked \"What's the weather in Boston and NYC?\", it could:\n",
    "\n",
    "Call weather tool for Boston\n",
    "Return to agent\n",
    "Call weather tool for NYC\n",
    "Return to agent\n",
    "Synthesize both results into a final response\n",
    "\n",
    "This creates a flexible, iterative problem-solving loop rather than a rigid linear flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cdf3677-5fba-4ba3-a018-5652267df685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(\"agent\", call_model)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    lambda x: \"tools\" if x[\"messages\"][-1].tool_calls else \"__end__\"\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"agent\")\n",
    "graph_builder.set_entry_point(\"agent\")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f8b420-62cd-48b8-9a9a-a1ddb6d4aefd",
   "metadata": {},
   "source": [
    "# üß™ Testing the Agent's Tool-Calling Ability\n",
    "This code demonstrates how to use our agent and verify that it's actually using tools rather than just generating responses.\n",
    "\n",
    "## What's happening here:\n",
    "```python\n",
    "# Test the agent's tool calling\n",
    "result = graph.invoke({\n",
    "    \"messages\": [(\"user\", \"What's the weather in Boston?\")]\n",
    "})\n",
    "print(result[\"messages\"][-1].content)\n",
    "\n",
    "# Verify that tool call actually occurred\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f\"\\nType: {type(msg).__name__}\")\n",
    "    print(f\"Content: {msg.content}\")\n",
    "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "        print(f\"Tool calls: {msg.tool_calls}\")\n",
    "```\n",
    "\n",
    "## Breaking it down:\n",
    "## Part 1: Invoking the Agent\n",
    "\n",
    "* graph.invoke(...): Starts the agent workflow\n",
    "\n",
    "    * Input format: Dictionary with \"messages\" key\n",
    "    * Message format: Tuple of (\"user\", \"question\")\n",
    "    * This kicks off the entire agent‚Üítools‚Üíagent flow\n",
    "\n",
    "\n",
    "* result[\"messages\"][-1].content: Gets the final response\n",
    "\n",
    "    * The result contains ALL messages from the conversation\n",
    "    * [-1] gets the last message (the agent's final response)\n",
    "    * This is what the user would actually see\n",
    "\n",
    "\n",
    "\n",
    "## Part 2: Debugging & Verification\n",
    "The second loop shows us the complete conversation flow:\n",
    "```python\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f\"\\nType: {type(msg).__name__}\")\n",
    "    print(f\"Content: {msg.content}\")\n",
    "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "        print(f\"Tool calls: {msg.tool_calls}\")\n",
    "```\n",
    "\n",
    "This reveals the behind-the-scenes process:\n",
    "\n",
    "1. **HumanMessage**: \"What's the weather in Boston?\"\n",
    "2. **AIMessage**: Contains tool_calls for get_weather\n",
    "3. **ToolMessage**: The actual weather data returned\n",
    "4. **AIMessage**: Final natural language response\n",
    "\n",
    "### Expected Output:\n",
    "```\n",
    "The weather in Boston is currently 72¬∞F and sunny with clear skies!\n",
    "\n",
    "Type: HumanMessage\n",
    "Content: What's the weather in Boston?\n",
    "\n",
    "Type: AIMessage\n",
    "Content: \n",
    "Tool calls: [{'name': 'get_weather', 'args': {'location': 'Boston'}}]\n",
    "\n",
    "Type: ToolMessage\n",
    "Content: Current weather in Boston: Temperature is 72¬∞F, conditions are sunny with clear skies\n",
    "\n",
    "Type: AIMessage\n",
    "Content: The weather in Boston is currently 72¬∞F and sunny with clear skies!\n",
    "```\n",
    "\n",
    "## üí° Why this verification matters:\n",
    "\n",
    "Confirms tool usage: We can see the agent actually called the tool rather than hallucinating weather data\n",
    "Debugging aid: If something goes wrong, we can see exactly where in the flow it failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485833ce-0550-4ede-abdf-3f5b75d53d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The weather in Boston is currently sunny with clear skies and the temperature is 72¬∞F. Enjoy your day!\n",
      "\n",
      "Type: HumanMessage\n",
      "Content: What's the weather in Boston?\n",
      "\n",
      "Type: AIMessage\n",
      "Content: \n",
      "Tool calls: [{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'yriRmzvtR', 'type': 'tool_call'}]\n",
      "\n",
      "Type: ToolMessage\n",
      "Content: Current weather in Boston: Temperature is 72¬∞F, conditions are sunny with clear skies\n",
      "\n",
      "Type: AIMessage\n",
      "Content:  The weather in Boston is currently sunny with clear skies and the temperature is 72¬∞F. Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "# Test the agent's tool calling\n",
    "result = graph.invoke({\n",
    "    \"messages\": [(\"user\", \"What's the weather in Boston?\")]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)\n",
    "\n",
    "# verify that tool call actually occurred\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f\"\\nType: {type(msg).__name__}\")\n",
    "    print(f\"Content: {msg.content}\")\n",
    "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "        print(f\"Tool calls: {msg.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5538b4-830b-4673-9c2d-7a08b080cde5",
   "metadata": {},
   "source": [
    "# üöÄ Next Steps: From Mock to Production\n",
    "Now that you understand the fundamentals of tool-calling agents, here's how to evolve this into a production-ready system:\n",
    "\n",
    "## Immediate Enhancements:\n",
    "\n",
    "1. Replace Mock Tools with Real APIs\n",
    "\n",
    "    * Integrate actual weather APIs (OpenWeatherMap, Weather.gov)\n",
    "    * Add error handling for API failures and rate limits\n",
    "    * Implement retry logic and fallback strategies\n",
    "\n",
    "\n",
    "2. Expand the Toolset\n",
    "\n",
    "    * Add complementary tools (forecast, weather alerts, historical weather)\n",
    "    * Create tools for different domains (news, calculations, web search)\n",
    "    * Build tool chains where one tool's output feeds into another\n",
    "\n",
    "\n",
    "3. Enhance Agent Intelligence\n",
    "\n",
    "    * Add memory/context persistence across conversations\n",
    "    * Implement planning capabilities for multi-step problems\n",
    "    * Fine-tune prompts for better tool selection decisions\n",
    "\n",
    "## Advanced Patterns:\n",
    "\n",
    "* Tool Validation: Add parameter validation before tool execution\n",
    "* Parallel Execution: Configure the agent to call multiple tools simultaneously\n",
    "* Observability: Add logging and monitoring to track tool usage and performance\n",
    "* Human-in-the-Loop: Build approval workflows for sensitive tool operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9a347-91d1-45da-b22f-4ba1d67ffec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
