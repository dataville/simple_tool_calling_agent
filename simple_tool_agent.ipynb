{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool-Calling Agent with Hybrid Evaluation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete tool-calling AI agent system built with LangChain and LangGraph. The agent can execute function calls (weather queries, calculations) and includes semantic evaluation of its responses.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The system uses **one LLM instance** (Hermes-3-Llama-3.1-8B via vLLM) serving two distinct roles:\n",
    "\n",
    "- **Agent Mode**: LLM with tools bound (`llm_with_tools`) - makes structured tool calls\n",
    "- **Evaluator Mode**: Raw LLM without tools (`llm`) - performs semantic assessment\n",
    "\n",
    "This hybrid approach addresses GPU memory constraints while maintaining proper separation between agent execution and evaluation logic.\n",
    "\n",
    "## Key Capabilities\n",
    "\n",
    "- **Structured Tool Calling**: Agent makes actual function calls rather than describing tool usage in text\n",
    "- **Smart Routing**: Uses LangGraph to manage agent flow and decide when to call tools vs. return final answers\n",
    "- **Semantic Evaluation**: LLM-based assessment of tool selection correctness and response quality, avoiding brittle keyword matching\n",
    "- **Input Validation**: Pydantic schemas ensure proper tool parameter handling\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- vLLM server running Hermes-3-Llama-3.1-8B on port 8082\n",
    "- Function-calling capable model (general instruction models lack this capability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import ToolMessage\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to vLLM Server\n",
    "\n",
    "**Prerequisites**: Before running this notebook, start the vLLM server in a separate terminal window.\n",
    "\n",
    "### Starting the Server\n",
    "\n",
    "Open a terminal and run:\n",
    "```bash\n",
    "vllm serve NousResearch/Hermes-3-Llama-3.1-8B \\\n",
    "  --port 8082 \\\n",
    "  --enable-auto-tool-choice \\\n",
    "  --tool-call-parser hermes \\\n",
    "  --max-model-len 8192\n",
    "```\n",
    "\n",
    "**Required flags explained**:\n",
    "- `--enable-auto-tool-choice`: Enables automatic tool calling capability\n",
    "- `--tool-call-parser hermes`: Uses Hermes-specific parser for structured tool calls\n",
    "- `--max-model-len 8192`: Limits context window to fit in 24GB GPU memory\n",
    "\n",
    "Wait for the server to fully load (you'll see \"Application startup complete\" in the logs) before proceeding.\n",
    "\n",
    "### Connection Setup\n",
    "\n",
    "This cell creates the base LLM connection that will be used for both agent operations (with tools bound) and evaluation (without tools). The `api_key` parameter is required by the OpenAI client interface but not validated by vLLM for local connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connected to vLLM server on port 8082\n"
     ]
    }
   ],
   "source": [
    "# Connect to vLLM - this is the BASE LLM (no tools)\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8082/v1\",\n",
    "    api_key=\"not-needed\",\n",
    "    model=\"NousResearch/Hermes-3-Llama-3.1-8B\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Connected to vLLM server on port 8082\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic Models for Tool Input Validation\n",
    "\n",
    "Pydantic models define the expected input schema for each tool, ensuring the LLM generates valid, structured tool calls.\n",
    "\n",
    "### Critical Requirement: Function-Calling Capable Models\n",
    "\n",
    "**Not all LLMs support Pydantic-based tool calling.** The model must be specifically trained for structured function calling. \n",
    "\n",
    "**Before selecting an LLM**, verify it explicitly supports function calling with structured schemas. Using a model without this capability will result in the agent describing tool usage in text rather than making actual structured tool calls.\n",
    "\n",
    "### Why Pydantic?\n",
    "\n",
    "- **Type Safety**: Enforces correct data types (str, float, int, etc.)\n",
    "- **Validation**: Applies constraints (min/max length, numerical ranges)\n",
    "- **Schema Generation**: Automatically creates JSON schemas that the LLM uses to structure tool calls\n",
    "- **Field Descriptions**: Provides detailed parameter descriptions that guide the LLM's tool usage\n",
    "\n",
    "### Pattern Used\n",
    "\n",
    "Each tool follows this pattern:\n",
    "\n",
    "1. Define a Pydantic `BaseModel` subclass with typed fields\n",
    "2. Use `Field()` to add descriptions and validation constraints\n",
    "3. Pass the model to `@tool(args_schema=ModelClass)` decorator\n",
    "4. Write the function with a complete docstring\n",
    "\n",
    "The field descriptions are critical‚Äîthey're sent to the LLM and directly influence how it constructs tool calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "Tools are Python functions that the agent can call to perform specific tasks. The LLM must understand what each tool does and when to use it based solely on the tool's metadata.\n",
    "\n",
    "### Model Selection for Tool Calling\n",
    "\n",
    "**Critical requirement**: You must use a model specifically trained for function calling. Not all LLMs support structured tool calls.\n",
    "\n",
    "**What happens with the wrong model**: The agent will generate text descriptions like \"I would call the weather tool for Boston\" instead of making actual structured function calls. Always verify function-calling support before implementing your agent.\n",
    "\n",
    "### Critical Role of Docstrings\n",
    "\n",
    "**Docstrings are not optional**‚Äîthey are the primary mechanism by which the LLM learns what each tool does. The docstring content is sent to the LLM as part of the tool schema. Many coding agents will skip the docstring or provide a minimal tool description that is not adequate for an agent to utilize.\n",
    "\n",
    "**What makes an effective tool docstring:**\n",
    "- **Clear purpose statement**: First line explains what the tool does\n",
    "- **Detailed parameter descriptions**: Each argument needs explanation and examples\n",
    "- **Return value documentation**: Describes what the tool outputs\n",
    "- **Format and constraints**: Any limitations or expected formats\n",
    "\n",
    "The LLM reads these docstrings to decide:\n",
    "1. Which tool (if any) to call for a given query\n",
    "2. What arguments to pass to the tool\n",
    "3. How to interpret the tool's output\n",
    "\n",
    "Poor or missing docstrings lead to incorrect tool selection and malformed tool calls. The quality of your docstrings directly determines agent reliability.\n",
    "\n",
    "### Tool Structure\n",
    "\n",
    "Each tool combines four components:\n",
    "1. **Pydantic model** (`args_schema`) - validates inputs\n",
    "2. **@tool decorator** - registers the function as a callable tool\n",
    "3. **Docstring** - instructs the LLM on tool usage\n",
    "4. **Function implementation** - executes the actual logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Tool\n",
    "\n",
    "**Pydantic Model**: `WeatherInput` defines a single required parameter:\n",
    "- `location` (str): City name with optional state/country\n",
    "- Validation: 2-100 characters in length\n",
    "- Field description provides format examples that guide the LLM's input generation\n",
    "\n",
    "**Tool Decorator**: `@tool(args_schema=WeatherInput)` binds the validation schema to the function, ensuring all calls pass through Pydantic validation before execution.\n",
    "\n",
    "**Docstring**: Provides structured documentation that the LLM uses to understand:\n",
    "- **Purpose**: What the tool retrieves (current weather information)\n",
    "- **Parameters**: Repeats the location format with examples\n",
    "- **Returns**: Expected output format (temperature + conditions)\n",
    "\n",
    "**Implementation**: Returns a mock weather response. In production, this would call an actual weather API, but for demonstration purposes returns a fixed sunny/72¬∞F response with the requested location interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for input validation\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str = Field(\n",
    "        description=\"The city name and optionally state/country (e.g., 'San Francisco, CA' or 'London, UK')\",\n",
    "        min_length=2,\n",
    "        max_length=100\n",
    "    )\n",
    "\n",
    "# Define a tool with Pydantic validation\n",
    "@tool(args_schema=WeatherInput)\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves current weather information for a specified location.\n",
    "    \n",
    "    Args:\n",
    "        location: The city name and optionally state/country (e.g., \"San Francisco, CA\" or \"London, UK\")\n",
    "    \n",
    "    Returns:\n",
    "        A string containing the current temperature in Fahrenheit and weather conditions.\n",
    "    \"\"\"\n",
    "    return f\"Current weather in {location}: Temperature is 72¬∞F, conditions are sunny with clear skies\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculator Tool\n",
    "\n",
    "**Pydantic Model**: `CalculatorInput` defines three required parameters:\n",
    "- `operation` (str): Specifies which mathematical operation to perform\n",
    "- `a` (float): First operand\n",
    "- `b` (float): Second operand\n",
    "- Field descriptions clearly enumerate the four supported operations, helping the LLM choose valid operation names\n",
    "\n",
    "**Tool Decorator**: `@tool(args_schema=CalculatorInput)` binds the validation schema, ensuring type correctness (floats for numbers, valid string for operation).\n",
    "\n",
    "**Docstring**: Documents the tool's purpose and parameter meanings, enabling the LLM to correctly map user queries like \"multiply 25 by 4\" to the appropriate operation name and operands.\n",
    "\n",
    "**Implementation**: \n",
    "- Executes basic arithmetic using if/elif branching\n",
    "- **Error handling**: Returns descriptive error messages for division by zero and unknown operations\n",
    "- **Return format**: Provides a natural language response incorporating the operation name and result\n",
    "- This pattern (string returns vs. raw numbers) allows the agent to easily incorporate results into conversational responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for calculator input\n",
    "class CalculatorInput(BaseModel):\n",
    "    operation: str = Field(\n",
    "        description=\"The mathematical operation: 'add', 'subtract', 'multiply', or 'divide'\"\n",
    "    )\n",
    "    a: float = Field(description=\"First number\")\n",
    "    b: float = Field(description=\"Second number\")\n",
    "\n",
    "# Define calculator tool with Pydantic validation\n",
    "@tool(args_schema=CalculatorInput)\n",
    "def calculator(operation: str, a: float, b: float) -> str:\n",
    "    \"\"\"\n",
    "    Performs basic mathematical operations on two numbers.\n",
    "    \n",
    "    Args:\n",
    "        operation: The mathematical operation to perform (add, subtract, multiply, divide)\n",
    "        a: First number\n",
    "        b: Second number\n",
    "    \n",
    "    Returns:\n",
    "        A string containing the result of the operation.\n",
    "    \"\"\"\n",
    "    if operation == \"add\":\n",
    "        result = a + b\n",
    "    elif operation == \"subtract\":\n",
    "        result = a - b\n",
    "    elif operation == \"multiply\":\n",
    "        result = a * b\n",
    "    elif operation == \"divide\":\n",
    "        if b == 0:\n",
    "            return \"Error: Cannot divide by zero\"\n",
    "        result = a / b\n",
    "    else:\n",
    "        return f\"Error: Unknown operation '{operation}'\"\n",
    "    \n",
    "    return f\"The result of {operation}ing {a} and {b} is {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bind Tools to Create Agent LLM\n",
    "\n",
    "**This is a critical step that many coding agents incorrectly skip.** Simply defining tools is not enough‚Äîthey must be explicitly bound to the LLM.\n",
    "\n",
    "### What Tool Binding Does\n",
    "\n",
    "`llm.bind_tools(tools)` creates a **new LLM instance** (`llm_with_tools`) that:\n",
    "\n",
    "1. **Receives tool schemas**: The Pydantic models and docstrings are converted to JSON schemas and sent with every request\n",
    "2. **Enables structured output**: The LLM is configured to return tool calls as structured data (function name + arguments) rather than text descriptions\n",
    "3. **Maintains the original**: The base `llm` remains unchanged and available for non-tool tasks\n",
    "\n",
    "### Why This Step is Essential\n",
    "\n",
    "**Without binding:**\n",
    "- The LLM has no knowledge of available tools\n",
    "- Responses will be text descriptions like \"I would call the weather tool...\" instead of actual function calls\n",
    "- The agent cannot execute any tools\n",
    "\n",
    "**With binding:**\n",
    "- The LLM receives tool metadata with every prompt\n",
    "- It can generate properly structured tool calls: `{\"name\": \"get_weather\", \"args\": {\"location\": \"Boston\"}}`\n",
    "- The agent can execute the structured calls and return real results\n",
    "\n",
    "### Two LLM Instances in This System\n",
    "\n",
    "- `llm`: Base model without tools (used for evaluation)\n",
    "- `llm_with_tools`: Agent model with tools bound (used for agent responses)\n",
    "\n",
    "Both reference the same underlying model server, but `llm_with_tools` includes tool schema metadata in its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tools bound to LLM\n",
      "  Available tools: ['get_weather', 'calculator']\n"
     ]
    }
   ],
   "source": [
    "# Bind tools to create the AGENT version of the LLM\n",
    "tools = [get_weather, calculator]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"‚úì Tools bound to LLM\")\n",
    "print(f\"  Available tools: {[tool.name for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Node: call_model Function\n",
    "\n",
    "This function serves as the agent node, responsible for generating responses and deciding when to call tools.\n",
    "\n",
    "**System Message Design**: The system prompt provides behavioral instructions but **deliberately does not list available tools**. This is correct‚Äîtool information is already included in `llm_with_tools` via the bound schemas. Listing tools in the system message would be redundant and could confuse the LLM with duplicate information.\n",
    "\n",
    "**Key Instructions in System Prompt:**\n",
    "- Identifies the assistant's role\n",
    "- Enforces tool-first behavior: \"You MUST call a tool to answer questions\"\n",
    "- Provides fallback response for out-of-scope queries\n",
    "\n",
    "**Message Construction**: Combines the system message with the conversation history from `state[\"messages\"]`, maintaining full context across turns.\n",
    "\n",
    "**Critical LLM Selection**: Uses `llm_with_tools` (not base `llm`) to invoke the model. This ensures:\n",
    "- Tool schemas are included in the request\n",
    "- The model can generate structured tool calls\n",
    "- Proper function calling behavior is enabled\n",
    "\n",
    "**Tool Usage Enforcement Logic:**\n",
    "1. Checks if any `ToolMessage` exists in conversation history\n",
    "2. Checks if current response contains tool calls\n",
    "3. If neither condition is met, overrides response with \"I don't have a tool to answer that question.\"\n",
    "\n",
    "This enforcement prevents the agent from attempting to answer questions without using tools, maintaining consistency with the tool-first design pattern. Once a tool has been used in the conversation, the agent can provide synthesis responses using those results.\n",
    "\n",
    "**Return Value**: Returns updated state with the new message appended to the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    \"\"\"\n",
    "    Agent node - uses llm_with_tools for tool calling.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_message = f\"\"\"You are a helpful assistant with access to specific tools.\n",
    "\n",
    "You MUST call a tool to answer questions. If no tool is available for the user's question, respond with:\n",
    "'I don't have a tool to answer that question.'\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + state[\"messages\"]\n",
    "    \n",
    "    # CRITICAL: Use llm_with_tools for agent responses\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Check if ANY tool was called in this conversation\n",
    "    tool_was_used = any(isinstance(msg, ToolMessage) for msg in state[\"messages\"])\n",
    "    \n",
    "    # Only force \"can't answer\" if:\n",
    "    # 1. No tool calls in current response AND\n",
    "    # 2. No tool was used earlier in conversation\n",
    "    if (not hasattr(response, 'tool_calls') or not response.tool_calls) and not tool_was_used:\n",
    "        response.content = \"I don't have a tool to answer that question.\"\n",
    "    \n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Agent Graph\n",
    "\n",
    "**Graph Initialization**: Creates a `StateGraph` with `MessagesState` as the state schema. `MessagesState` automatically manages the conversation history, appending new messages to the `messages` list.\n",
    "\n",
    "**Agent Node**: `add_node(\"agent\", call_model)` registers the `call_model` function as a node. This node receives the current state, invokes the LLM, and returns updated state with the response.\n",
    "\n",
    "**Tools Node**: `add_node(\"tools\", ToolNode(tools))` creates a pre-built node that automatically:\n",
    "- Executes tool calls from the agent's response\n",
    "- Handles multiple tool calls if present\n",
    "- Returns `ToolMessage` objects with execution results\n",
    "- Manages errors if tool execution fails\n",
    "\n",
    "**Conditional Routing**: `add_conditional_edges()` examines the last message in state and routes based on presence of tool calls:\n",
    "- If `tool_calls` exists ‚Üí route to \"tools\" node for execution\n",
    "- If no `tool_calls` ‚Üí route to `END` to complete the workflow\n",
    "\n",
    "This lambda function determines the graph's flow: tools needed vs. final answer ready.\n",
    "\n",
    "**Return Edge**: `add_edge(\"tools\", \"agent\")` creates an unconditional edge from tools back to agent. After tools execute, control always returns to the agent to synthesize results into a natural language response.\n",
    "\n",
    "**Entry Point**: `add_edge(START, \"agent\")` defines where execution begins. Every invocation starts at the agent node.\n",
    "\n",
    "**Compilation**: `compile()` validates the graph structure and creates an executable workflow. The compiled graph can be invoked with initial state.\n",
    "\n",
    "**Graph Flow Summary**: `START` ‚Üí `agent` ‚Üí (conditional) ‚Üí `tools` ‚Üí `agent` ‚Üí (conditional) ‚Üí `END`\n",
    "\n",
    "### Important: Watch for Outdated Syntax\n",
    "\n",
    "**Warning for users working with AI coding assistants**: Many LLM-based coding agents (including older versions of Claude, ChatGPT, and others) will generate LangGraph code using the **legacy API** with `set_entry_point()` and `\"__end__\"` instead of the current `START` and `END` keywords.\n",
    "\n",
    "**Legacy syntax (outdated):**\n",
    "```python\n",
    "graph_builder.set_entry_point(\"agent\")  # Old way\n",
    "lambda x: \"tools\" if x[\"messages\"][-1].tool_calls else \"__end__\"  # Old way\n",
    "```\n",
    "\n",
    "**Current syntax (used in this notebook):**\n",
    "```python\n",
    "graph_builder.add_edge(START, \"agent\")  # Current way\n",
    "lambda x: \"tools\" if x[\"messages\"][-1].tool_calls else END  # Current way\n",
    "```\n",
    "\n",
    "**Why this matters**: The legacy syntax still works but is deprecated. If an AI assistant generates graph code for you, verify it uses `START` and `END` keywords. If it uses the old syntax, update it to match the current API shown in this notebook.\n",
    "\n",
    "**What to do**: When copying graph building code from AI assistants, always check for and update these patterns to use the current LangGraph API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agent graph compiled\n"
     ]
    }
   ],
   "source": [
    "# Build graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(\"agent\", call_model)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools))\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    lambda x: \"tools\" if x[\"messages\"][-1].tool_calls else END\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"agent\")\n",
    "graph_builder.add_edge(START, \"agent\")\n",
    "graph = graph_builder.compile()\n",
    "print(\"‚úì Agent graph compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Evaluator Functions\n",
    "\n",
    "This evaluator uses **the same LLM instance** for assessment but without tools bound, implementing a \"hybrid\" approach that addresses GPU memory constraints.\n",
    "\n",
    "### Why \"Hybrid\"?\n",
    "\n",
    "The term \"hybrid\" refers to using one physical model instance in two different modes:\n",
    "- **Agent mode**: `llm_with_tools` (with tool schemas)\n",
    "- **Evaluator mode**: `llm` (raw model, no tools)\n",
    "\n",
    "This allows semantic evaluation without requiring a second model to be loaded into GPU memory.\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "Rather than brittle keyword matching (e.g., checking if response contains \"multiply\"), the evaluator uses the LLM's semantic understanding to assess:\n",
    "\n",
    "1. **Tool Selection Correctness**: Did the agent choose appropriate tools for the query?\n",
    "2. **Response Quality**: Is the final answer clear, complete, and well-formatted?\n",
    "3. **Overall Success**: Does the response successfully address the user's question?\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The `evaluate_response` function:\n",
    "- Constructs a structured evaluation prompt with available tools, tools used, and final answer\n",
    "- Invokes the **raw LLM** (without tool bindings) to perform assessment\n",
    "- Parses the LLM's evaluation into structured fields\n",
    "- Returns a dictionary with ratings and reasoning for each criterion\n",
    "\n",
    "This approach provides nuanced evaluation that understands semantic variations (e.g., \"multiplied\" vs \"multiply\" vs \"times\") without fragile string matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(question: str, final_answer: str, tool_calls: list, messages: list, available_tools: list) -> dict:\n",
    "    \"\"\"\n",
    "    LLM-based evaluation of agent performance.\n",
    "    Uses the LLM to assess all aspects of the response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build tool descriptions for context\n",
    "    tool_descriptions = \"\\n\".join([\n",
    "        f\"- {tool.name}: {tool.description}\"\n",
    "        for tool in available_tools\n",
    "    ])\n",
    "    \n",
    "    tools_used = [tc['name'] for tc in tool_calls] if tool_calls else []\n",
    "    \n",
    "    eval_prompt = f\"\"\"You are evaluating an AI agent's performance.\n",
    "\n",
    "AVAILABLE TOOLS:\n",
    "{tool_descriptions}\n",
    "\n",
    "USER QUESTION: {question}\n",
    "\n",
    "TOOLS USED BY AGENT: {tools_used if tools_used else \"None\"}\n",
    "\n",
    "AGENT'S FINAL ANSWER: {final_answer}\n",
    "\n",
    "Evaluate the agent's performance on these criteria:\n",
    "\n",
    "1. TOOL SELECTION: Did the agent correctly identify whether tools were needed? If tools were needed, did it select the appropriate tool(s)?\n",
    "   - \"correct\" if tool usage was appropriate\n",
    "   - \"incorrect\" if wrong tool was used or tools were used unnecessarily\n",
    "   - \"missing\" if tools were needed but not used\n",
    "\n",
    "2. RESPONSE QUALITY: Is the final answer clear, complete, and appropriately formatted?\n",
    "   - \"good\" if answer is clear, complete, and well-formatted\n",
    "   - \"fair\" if answer is acceptable but could be improved\n",
    "   - \"poor\" if answer is unclear, incomplete, or poorly formatted\n",
    "\n",
    "3. OVERALL: Does the response successfully address the user's question?\n",
    "   - \"pass\" if the agent handled the question correctly\n",
    "   - \"fail\" if there were significant errors in tool usage or response quality\n",
    "\n",
    "Respond in this EXACT format:\n",
    "TOOL_SELECTION: correct/incorrect/missing\n",
    "TOOL_REASONING: <brief explanation of tool selection>\n",
    "QUALITY: good/fair/poor\n",
    "QUALITY_REASONING: <brief explanation of response quality>\n",
    "OVERALL: pass/fail\n",
    "OVERALL_REASONING: <brief summary>\n",
    "\"\"\"\n",
    "    \n",
    "    # Use raw LLM for evaluation\n",
    "    eval_response = llm.invoke(eval_prompt)\n",
    "    eval_text = eval_response.content\n",
    "    \n",
    "    # Parse response\n",
    "    result = {\n",
    "        'tool_selection': None,\n",
    "        'tool_reasoning': None,\n",
    "        'quality': None,\n",
    "        'quality_reasoning': None,\n",
    "        'overall': None,\n",
    "        'overall_reasoning': None,\n",
    "        'raw_evaluation': eval_text\n",
    "    }\n",
    "    \n",
    "    for line in eval_text.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line.startswith('TOOL_SELECTION:'):\n",
    "            result['tool_selection'] = line.replace('TOOL_SELECTION:', '').strip().lower()\n",
    "        elif line.startswith('TOOL_REASONING:'):\n",
    "            result['tool_reasoning'] = line.replace('TOOL_REASONING:', '').strip()\n",
    "        elif line.startswith('QUALITY:'):\n",
    "            result['quality'] = line.replace('QUALITY:', '').strip().lower()\n",
    "        elif line.startswith('QUALITY_REASONING:'):\n",
    "            result['quality_reasoning'] = line.replace('QUALITY_REASONING:', '').strip()\n",
    "        elif line.startswith('OVERALL:'):\n",
    "            result['overall'] = line.replace('OVERALL:', '').strip().lower()\n",
    "        elif line.startswith('OVERALL_REASONING:'):\n",
    "            result['overall_reasoning'] = line.replace('OVERALL_REASONING:', '').strip()\n",
    "    \n",
    "    # Validate that parsing succeeded\n",
    "    if not all([result['tool_selection'], result['quality'], result['overall']]):\n",
    "        print(f\"WARNING: Failed to parse evaluation response:\")\n",
    "        print(eval_text)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Agent with Full Evaluation\n",
    "\n",
    "This section runs the complete agent workflow and evaluation in a single pass, showing both the internal message flow and the evaluation results.\n",
    "\n",
    "**Test Cases**: Three questions that demonstrate different agent behaviors:\n",
    "1. Weather query - requires `get_weather` tool\n",
    "2. Math query - requires `calculator` tool  \n",
    "3. General knowledge - no applicable tool (should decline gracefully)\n",
    "\n",
    "**Output Structure**: For each test case, displays:\n",
    "\n",
    "1. **Message Flow**: Shows the complete conversation with message types:\n",
    "   - `HumanMessage`: User's question\n",
    "   - `AIMessage` (with tool_calls): Agent's decision to call a tool\n",
    "   - `ToolMessage`: Tool execution result\n",
    "   - `AIMessage` (final): Agent's synthesized natural language response\n",
    "\n",
    "2. **Evaluation Results**: LLM-based assessment including:\n",
    "   - Tool Selection: Whether correct tools were chosen\n",
    "   - Response Quality: Clarity and completeness of final answer\n",
    "   - Overall Assessment: Pass/fail determination with reasoning\n",
    "\n",
    "**Why Combined Output**: Running both together ensures consistency‚Äîthe evaluation analyzes the exact same execution that generated the message flow, eliminating any discrepancies from separate runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AGENT TESTING WITH EVALUATION\n",
      "Using single LLM (Hermes-3-Llama-3.1-8B on port 8082)\n",
      "  - Agent uses llm_with_tools\n",
      "  - Evaluator uses raw llm\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Question: What's the weather in Boston?\n",
      "======================================================================\n",
      "\n",
      "üîÑ MESSAGE FLOW:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Type: HumanMessage\n",
      "Content: What's the weather in Boston?\n",
      "\n",
      "Type: AIMessage\n",
      "Content: \n",
      "Tool calls: [{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'chatcmpl-tool-ec5485a7d5f1498fb6277a190582559a', 'type': 'tool_call'}]\n",
      "\n",
      "Type: ToolMessage\n",
      "Content: Current weather in Boston: Temperature is 72¬∞F, conditions are sunny with clear skies\n",
      "\n",
      "Type: AIMessage\n",
      "Content: The current temperature in Boston is 72¬∞F and the weather conditions are sunny with clear skies.\n",
      "\n",
      "======================================================================\n",
      "üìä EVALUATION RESULTS:\n",
      "======================================================================\n",
      "Overall: ‚úÖ PASS\n",
      "\n",
      "üîß Tool Selection:\n",
      "   Status: correct\n",
      "   Reasoning: The agent correctly identified that the 'get_weather' tool was needed to retrieve the weather information for Boston.\n",
      "\n",
      "‚≠ê Response Quality:\n",
      "   Quality: good\n",
      "   Reasoning: The agent's final answer is clear, complete, and well-formatted, providing both the current temperature and weather conditions.\n",
      "\n",
      "üìù Overall Assessment:\n",
      "   The agent successfully addressed the user's question by using the appropriate tool and providing a good quality response.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Question: What is 25 multiplied by 4?\n",
      "======================================================================\n",
      "\n",
      "üîÑ MESSAGE FLOW:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Type: HumanMessage\n",
      "Content: What is 25 multiplied by 4?\n",
      "\n",
      "Type: AIMessage\n",
      "Content: \n",
      "Tool calls: [{'name': 'calculator', 'args': {'operation': 'multiply', 'a': 25, 'b': 4}, 'id': 'chatcmpl-tool-86e35cf59a4b4811b957cafec35ad41d', 'type': 'tool_call'}]\n",
      "\n",
      "Type: ToolMessage\n",
      "Content: The result of multiplying 25.0 and 4.0 is 100.0\n",
      "\n",
      "Type: AIMessage\n",
      "Content: The result of multiplying 25 and 4 is 100.\n",
      "\n",
      "======================================================================\n",
      "üìä EVALUATION RESULTS:\n",
      "======================================================================\n",
      "Overall: ‚úÖ PASS\n",
      "\n",
      "üîß Tool Selection:\n",
      "   Status: correct\n",
      "   Reasoning: The agent correctly identified that the calculator tool was needed to perform the multiplication operation.\n",
      "\n",
      "‚≠ê Response Quality:\n",
      "   Quality: good\n",
      "   Reasoning: The agent provided a clear, complete, and well-formatted answer to the user's question.\n",
      "\n",
      "üìù Overall Assessment:\n",
      "   The agent successfully addressed the user's question by using the appropriate tool and providing a high-quality response.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Question: What is the capital of France?\n",
      "======================================================================\n",
      "\n",
      "üîÑ MESSAGE FLOW:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Type: HumanMessage\n",
      "Content: What is the capital of France?\n",
      "\n",
      "Type: AIMessage\n",
      "Content: I don't have a tool to answer that question.\n",
      "\n",
      "======================================================================\n",
      "üìä EVALUATION RESULTS:\n",
      "======================================================================\n",
      "Overall: ‚úÖ PASS\n",
      "\n",
      "üîß Tool Selection:\n",
      "   Status: missing\n",
      "   Reasoning: The agent did not need to use any tools to answer the question about the capital of France.\n",
      "\n",
      "‚≠ê Response Quality:\n",
      "   Quality: good\n",
      "   Reasoning: The answer is clear, complete, and appropriately formatted.\n",
      "\n",
      "üìù Overall Assessment:\n",
      "   The agent correctly determined that no tools were needed and provided a clear, complete, and well-formatted answer to the question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "test_cases = [\n",
    "    \"What's the weather in Boston?\",\n",
    "    \"What is 25 multiplied by 4?\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AGENT TESTING WITH EVALUATION\")\n",
    "print(\"Using single LLM (Hermes-3-Llama-3.1-8B on port 8082)\")\n",
    "print(\"  - Agent uses llm_with_tools\")\n",
    "print(\"  - Evaluator uses raw llm\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for question in test_cases:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    # Run the agent\n",
    "    result = graph.invoke({\"messages\": [(\"user\", question)]})\n",
    "    \n",
    "    # Display message flow\n",
    "    print(\"\\nüîÑ MESSAGE FLOW:\")\n",
    "    print(\"-\"*70)\n",
    "    for msg in result[\"messages\"]:\n",
    "        msg_type = type(msg).__name__\n",
    "        print(f\"\\nType: {msg_type}\")\n",
    "        print(f\"Content: {msg.content}\")\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            print(f\"Tool calls: {msg.tool_calls}\")\n",
    "    \n",
    "    # Get final answer and tool calls for evaluation\n",
    "    final_answer = result[\"messages\"][-1].content\n",
    "    \n",
    "    tool_calls = []\n",
    "    for msg in result[\"messages\"]:\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            tool_calls.extend(msg.tool_calls)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluation = evaluate_response(question, final_answer, tool_calls, result[\"messages\"], tools)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìä EVALUATION RESULTS:\")\n",
    "    print('='*70)\n",
    "    print(f\"Overall: {'‚úÖ PASS' if evaluation['overall'] == 'pass' else '‚ùå FAIL'}\")\n",
    "    \n",
    "    print(f\"\\nüîß Tool Selection:\")\n",
    "    print(f\"   Status: {evaluation['tool_selection']}\")\n",
    "    print(f\"   Reasoning: {evaluation['tool_reasoning']}\")\n",
    "    \n",
    "    print(f\"\\n‚≠ê Response Quality:\")\n",
    "    print(f\"   Quality: {evaluation['quality']}\")\n",
    "    print(f\"   Reasoning: {evaluation['quality_reasoning']}\")\n",
    "    \n",
    "    print(f\"\\nüìù Overall Assessment:\")\n",
    "    print(f\"   {evaluation['overall_reasoning']}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
